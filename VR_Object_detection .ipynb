{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import io\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read model weights and config \n",
    "yolo = cv2.dnn.readNet(\"model/trainedYolo.weights\", \"model/trainedYolo.cfg\")\n",
    "# Read class names file\n",
    "f = open(\"model/cocoClasses.names\" , \"r\")\n",
    "objectClasses = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary that designates the bounding box location to any of the 9 regions\n",
    "mapping = {  \n",
    "    0: \"Top left\",\n",
    "    1: \"Center Left\",\n",
    "    2: \"Bottom Left\",\n",
    "    3: \"Center Top\",\n",
    "    4: \"Center\",\n",
    "    5: \"Center Bottom\",\n",
    "    6: \"Top Right\",\n",
    "    7: \"Center Right\",\n",
    "    8: \"Bottom Right\"\n",
    "}\n",
    "\n",
    "# To compute the window block location of objects detected \n",
    "def disect(h, w, x1, y1, h1, w1):\n",
    "    \n",
    "    # disection of image in vertical direction(3 regions, 1, 3, 5)\n",
    "    div_ver = [x*h//6 for x in range(1, 6, 2)]\n",
    "    \n",
    "    # disection of image in horizontal direction(3 regions 1, 3, 5)\n",
    "    div_hor = [x*w//6 for x in range(1, 6, 2)]\n",
    "    \n",
    "    count = 0\n",
    "    mn = sys.maxsize\n",
    "    index = 0  # mapping index\n",
    "    \n",
    "    for x in div_hor:\n",
    "        for y in div_ver:\n",
    "            if math.sqrt((x-x1-w1/2)**2 + (y-y1-h1/2)**2) < mn:\n",
    "                mn = math.sqrt((x-x1-w1/2)**2 + (y-y1-h1/2)**2)\n",
    "                index = count\n",
    "            count += 1\n",
    "\n",
    "    return mapping[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img):\n",
    "\n",
    "    layers = yolo.getLayerNames()\n",
    "    outputLayerIndices = yolo.getUnconnectedOutLayers()\n",
    "    outputLayers = [layers[i[0] - 1] for i in outputLayerIndices]\n",
    "\n",
    "    # Reshaping image\n",
    "    img = cv2.resize(img, None, fx=0.9, fy=0.9)\n",
    "    height, width, channels = img.shape\n",
    "    blobs = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    \n",
    "    # Forward pass the image\n",
    "    yolo.setInput(blobs)\n",
    "    outputs = yolo.forward(outputLayers)\n",
    "    ot = np.array(outputs)\n",
    "    \n",
    "    objCounts = [1]*100\n",
    "    object_ids = []\n",
    "    possibilities = []\n",
    "    boxes = []\n",
    "\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            \n",
    "            scores = detection[5:]\n",
    "            # print('SCORE SHAPE = ',scores.shape)\n",
    "            object_id = np.argmax(scores)\n",
    "            # print('ObjectID = ', object_id)\n",
    "            possibility = scores[object_id]\n",
    "\n",
    "            # Keeping those BB with condidence scores more than 50%\n",
    "            if(possibility > 0.5):\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                object_ids.append(object_id)\n",
    "                possibilities.append(float(possibility))\n",
    "                boxes.append([x, y, w, h])\n",
    "\n",
    "    # Discarding multiple overlapping BB and considering the best fit ones \n",
    "    uniqueIndices = cv2.dnn.NMSBoxes(boxes, possibilities, 0.4, 0.6)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    object_list = []\n",
    "    h, w = img.shape[0:2]\n",
    "    output = []\n",
    "    counter = 1\n",
    "\n",
    "    # Processing bounding boxes of detected objects and generating verbose output from image\n",
    "    for i in range(len(boxes)):\n",
    "        if i in uniqueIndices:\n",
    "            x, y, w1, h1 = boxes[i]\n",
    "            label = str(objectClasses[object_ids[i]])\n",
    "            cv2.rectangle(img, (x , y), (x + w1, y + h1), (0, 255, 255), 2)\n",
    "            cv2.putText(img, label , (x , y - 10), font , 1 , (255, 255, 255), 2)\n",
    "            object_list.append([x, y, w1, h1, label])\n",
    "            \n",
    "            # Finding the block location in which the current object belongs\n",
    "            loc = disect(h, w, x, y, h1, w1)\n",
    "            # Verbose info of the detected object\n",
    "            output.append(f'{counter}: {label}{objCounts[object_ids[i]]} is in {loc}, ')\n",
    "            objCounts[object_ids[i]] += 1\n",
    "            counter += 1\n",
    "\n",
    "    if not object_list:\n",
    "        return [\"No objects found\",None]\n",
    "    else:\n",
    "        return [output,img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects detected:\n",
      "1: car1 is in Center Right, \n",
      "2: traffic light1 is in Top left, \n",
      "3: car2 is in Center Left, \n",
      "4: car3 is in Center, \n",
      "5: car4 is in Center Right, \n",
      "6: car5 is in Center Left, \n",
      "7: person1 is in Center, \n",
      "8: traffic light2 is in Top Right, \n",
      "9: traffic light3 is in Top Right, \n",
      "10: traffic light4 is in Top left, \n",
      "11: traffic light5 is in Center Left, \n",
      "12: car6 is in Center Left, \n"
     ]
    }
   ],
   "source": [
    "# Sample image from which objects are to be detected \n",
    "img = cv2.imread(\"images/sample2.jpg\");\n",
    "\n",
    "# res = [detected objects as list, processed image]\n",
    "res = predict(img)\n",
    "\n",
    "# Detected objects as verbose output \n",
    "print('Objects detected:')\n",
    "for obj in res[0]:\n",
    "    print(obj)\n",
    "\n",
    "# Processed image after applying bounding boxes\n",
    "cv2.imshow(\"image\", res[1])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
